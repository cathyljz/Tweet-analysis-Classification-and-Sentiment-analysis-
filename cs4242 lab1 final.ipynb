{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4242 Lab 1: Microblog Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Li Jiazhe (A0176576M)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import simplejson as json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run Basic Classifier (given code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading and process samples...\n",
      "The number of unique words in data set is 27516.\n",
      "The number of low frequency words is 12707.\n",
      "Preprocessing is completed\n"
     ]
    }
   ],
   "source": [
    "def rm_html_tags(str): \n",
    "    html_prog = re.compile(r'<[^>]+>',re.S) \n",
    "    return html_prog.sub('', str) \n",
    "\n",
    "def rm_html_escape_characters(str):\n",
    "    pattern_str = r'&quot;|&amp;|&lt;|&gt;|&nbsp;|&#34;|&#38;|&#60;|&#62;|&#160;|&#20284;|&#30524;|&#26684|&#43;|&#20540|&#23612;'\n",
    "    escape_characters_prog = re.compile(pattern_str, re.S)\n",
    "    return escape_characters_prog.sub('', str)\n",
    "\n",
    "def rm_at_user(str):\n",
    "    return re.sub(r'@[a-zA-Z_0-9]*', '', str)\n",
    "\n",
    "def rm_url(str): \n",
    "    return re.sub(r'http[s]?:[/+]?[a-zA-Z0-9_\\.\\/]*', '', str)\n",
    "\n",
    "def rm_repeat_chars(str):\n",
    "    return re.sub(r'(.)(\\1){2,}', r'\\1\\1', str)\n",
    "\n",
    "def rm_hashtag_symbol(str):\n",
    "    return re.sub(r'#', '', str)\n",
    "\n",
    "def replace_emoticon(emoticon_dict, str):\n",
    "    for k, v in emoticon_dict.items():\n",
    "        str = str.replace(k, v)\n",
    "    return str\n",
    "\n",
    "def rm_time(str):\n",
    "    return re.sub(r'[0-9][0-9]:[0-9][0-9]', '', str)\n",
    "\n",
    "def rm_punctuation(current_tweet):\n",
    "    return re.sub(r'[^\\w\\s]','',current_tweet)\n",
    "\n",
    "\n",
    "def pre_process(str, porter):\n",
    "    # do not change the preprocessing order only if you know what you're doing \n",
    "    str = str.lower()\n",
    "    str = rm_url(str)        \n",
    "    str = rm_at_user(str)        \n",
    "    str = rm_repeat_chars(str) \n",
    "    str = rm_hashtag_symbol(str)       \n",
    "    str = rm_time(str)        \n",
    "    str = rm_punctuation(str)\n",
    "        \n",
    "    try:\n",
    "        str = nltk.tokenize.word_tokenize(str)\n",
    "        try:\n",
    "            str = [porter.stem(t) for t in str]\n",
    "        except:\n",
    "            print(str)\n",
    "            pass\n",
    "    except:\n",
    "        print(str)\n",
    "        pass\n",
    "        \n",
    "    return str\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = 'D:/Li Jiazhe/NUS/Semester 2/CS4242 Social Media Computing/LAB1/LAB1/data/data'  ##Setting your own file path here.\n",
    "\n",
    "    x_filename = 'samples.txt'\n",
    "    y_filename = 'labels.txt'\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stops.add('rt') \n",
    "\n",
    "\n",
    "    ##load and process samples\n",
    "    print('start loading and process samples...')\n",
    "    words_stat = {} \n",
    "    tweets = []\n",
    "    hashtags_all = []\n",
    "    name_all = []\n",
    "    cnt = 0\n",
    "    with open(os.path.join(data_dir, x_filename), encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            postprocess_tweet = []\n",
    "            tweet_obj = json.loads(line.strip(), encoding='utf-8') \n",
    "            description = tweet_obj['user']['description'].replace(\"\\n\",\" \")\n",
    "            content = tweet_obj['text'].replace(\"\\n\",\" \")\n",
    "            hashtags_dic = tweet_obj['entities']['hashtags']\n",
    "            hashtags = [d['text'] for d in hashtags_dic]\n",
    "            truncated_text = ['truncated_or_not_' + str(tweet_obj['truncated'])]\n",
    "            user_name = tweet_obj['user']['name']\n",
    "            text_words = pre_process(content, porter)\n",
    "            des_words = pre_process(description, porter)\n",
    "            user_name_words = pre_process(user_name, porter)\n",
    "            words = text_words + des_words + hashtags+ hashtags + truncated_text + user_name_words + hashtags\n",
    "            for word in words:\n",
    "                if word not in stops:\n",
    "                    postprocess_tweet.append(word)\n",
    "                    if word in words_stat.keys():\n",
    "                        words_stat[word][0] += 1 \n",
    "                        if i != words_stat[word][2]:\n",
    "                            words_stat[word][1] += 1\n",
    "                            words_stat[word][2] = i\n",
    "                    else:\n",
    "                        words_stat[word] = [1,1,i]\n",
    "            tweets.append(' '.join(postprocess_tweet))\n",
    "            \n",
    "            hashtags_all.append(' '.join(hashtags))\n",
    "            hashtags_all = ['standsfornull' if x == '' else x for x in hashtags_all ]\n",
    "            \n",
    "            name_all.append(' '.join(user_name_words))\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"The number of unique words in data set is %i.\" %len(words_stat.keys())) \n",
    "    # 12344 for text only, 19361 for adding user description, 24189 for adding hashtags, 24191 for adding truncated text\n",
    "    # 27516 for adding user name\n",
    "    lowTF_words = set()\n",
    "    with open(os.path.join(data_dir, 'words_statistics.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('TF\\tDF\\tWORD\\n')\n",
    "        for word, stat in sorted(words_stat.items(), key=lambda i: i[1], reverse=True):\n",
    "            f.write('\\t'.join([str(m) for m in stat[0:2]]) + '\\t' + word +  '\\n')\n",
    "            if stat[0]<2:\n",
    "                lowTF_words.add(word)\n",
    "    print(\"The number of low frequency words is %d.\" %len(lowTF_words))\n",
    "    # 7079 for text only, 11222 for adding user description, 13995 for adding hashtags, 13995 for adding truncated text\n",
    "    # 16028 for adding user name, 12707 for adding hashtags again.\n",
    "\n",
    "\n",
    "    ###Re-process samples, filter low frequency words...\n",
    "    fout = open(os.path.join(data_dir, 'samples_processed.txt'), 'w', encoding='utf-8')\n",
    "    tweets_new = []\n",
    "    for tweet in tweets:\n",
    "        words = tweet.split(' ')\n",
    "        new = [] \n",
    "        for w in words:\n",
    "            if (w not in lowTF_words) or (w in hashtags_all) or (w in name_all):\n",
    "                new.append(w)\n",
    "        new_tweet = ' '.join(new)\n",
    "        tweets_new.append(new_tweet)\n",
    "        fout.write('%s\\n' %new_tweet)\n",
    "    fout.close()\n",
    "    \n",
    "    print(\"Preprocessing is completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 14131)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data_dir = 'D:/Li Jiazhe/NUS/Semester 2/CS4242 Social Media Computing/LAB1/LAB1/data/data' \n",
    "\n",
    "# print(\"Loading data...\")\n",
    "with open(os.path.join(data_dir, 'samples_processed.txt'), 'r', encoding='utf-8') as f:\n",
    "\tx = f.readlines()\n",
    "with open(os.path.join(data_dir, 'labels.txt'), 'r', encoding='utf-8') as f:\n",
    "\ty = np.array(f.readlines())\n",
    "\n",
    "# print(\"Extract features...\")\n",
    "x_feats = TfidfVectorizer().fit_transform(x)\n",
    "print(x_feats.shape) # 5237 for text only, 8101 for adding user description, 9713 for adding hashtags, 9715 for adding \n",
    "# truncated text, 11830 for adding user name, 14131 for adding hashtags again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classifier: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision is 0.697134.\n",
      "Average Recall is 0.690572.\n",
      "F1 score is 0.689687.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# print(\"Start training and predict...\")\n",
    "fold = 10\n",
    "kf = KFold(n_splits=fold)\n",
    "avg_p = 0\n",
    "avg_r = 0\n",
    "avg_f1 = 0\n",
    "for train, test in kf.split(x_feats): # 10 rounds\n",
    "    model = MultinomialNB().fit(x_feats[train], y[train]) \n",
    "    predicts = model.predict(x_feats[test])\n",
    "    avg_p += precision_score(y[test],predicts, average='macro')\n",
    "    avg_r += recall_score(y[test],predicts, average='macro')\n",
    "    avg_f1 += f1_score(y[test],predicts, average='macro')\n",
    "\n",
    "nb_precision = avg_p/fold\n",
    "nb_recall = avg_r/fold\n",
    "nb_f1 = avg_f1/fold\n",
    "print('Average Precision is %f.' %(nb_precision))\n",
    "print('Average Recall is %f.' %(nb_recall))\n",
    "print('F1 score is %f.' %(nb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Try other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best k for knn is 12 and the optimal precision is 0.597731.\n",
      "Average Recall is 0.598760.\n",
      "F1 score is 0.597731.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# print(\"Start training and predict...\")\n",
    "fold = 10\n",
    "kf = KFold(n_splits=fold)\n",
    "K = [3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "avg_p_final = []\n",
    "avg_r_final = []\n",
    "avg_f1_final = []\n",
    "for k in K:\n",
    "    avg_p = 0\n",
    "    avg_r = 0\n",
    "    avg_f1 = 0\n",
    "    for train, test in kf.split(x_feats):\n",
    "        model = KNeighborsClassifier(n_neighbors=k).fit(x_feats[train], y[train])\n",
    "        predicts = model.predict(x_feats[test])\n",
    "#         print(classification_report(y[test],predicts))\n",
    "        avg_p += precision_score(y[test],predicts, average='macro')\n",
    "        avg_r += recall_score(y[test],predicts, average='macro')\n",
    "        avg_f1 += f1_score(y[test],predicts, average='macro')\n",
    "    avg_p_final.append(avg_p/fold)\n",
    "    avg_r_final.append(avg_r/fold)\n",
    "    avg_f1_final.append(avg_f1/fold)\n",
    "\n",
    "knn_precision = max(avg_f1_final)\n",
    "opt_k = K[avg_f1_final.index(knn_precision)]\n",
    "knn_recall = avg_r_final[avg_f1_final.index(knn_precision)]\n",
    "knn_f1 = avg_f1_final[avg_f1_final.index(knn_precision)]\n",
    "print('The best k for knn is %d and the optimal precision is %f.' % (opt_k, knn_precision))\n",
    "print('Average Recall is %f.' % knn_recall)\n",
    "print('F1 score is %f.' %(knn_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision of decision tree is 0.584852\n",
      "Average Recall is 0.521570.\n",
      "F1 score is 0.534734.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# print(\"Start training and predict...\")\n",
    "fold = 10\n",
    "kf = KFold(n_splits=fold)\n",
    "\n",
    "avg_p = 0\n",
    "avg_r = 0\n",
    "avg_f1 = 0\n",
    "for train, test in kf.split(x_feats):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth = 100)\n",
    "    model = dt.fit(x_feats[train], y[train])\n",
    "    predicts = model.predict(x_feats[test])\n",
    "#         print(classification_report(y[test],predicts))\n",
    "    avg_p += precision_score(y[test],predicts, average='macro')\n",
    "    avg_r += recall_score(y[test],predicts, average='macro')\n",
    "    avg_f1 += f1_score(y[test],predicts, average='macro')\n",
    "\n",
    "\n",
    "tree_precision = avg_p/fold\n",
    "tree_recall = avg_r/fold\n",
    "tree_f1 = avg_f1/fold\n",
    "print('Average Precision of decision tree is %f' % (tree_precision))\n",
    "print('Average Recall is %f.' % tree_recall)\n",
    "print('F1 score is %f.' %(tree_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision of RandomForest is 0.710513\n",
      "Average Recall is 0.708644.\n",
      "F1 score is 0.706380.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "avg_p = 0\n",
    "avg_r = 0\n",
    "avg_f1 = 0\n",
    "for train, test in kf.split(x_feats):\n",
    "    rf = RandomForestClassifier(n_estimators = 500, max_features = 7, random_state=0)\n",
    "    model = rf.fit(x_feats[train], y[train])\n",
    "    predicts = model.predict(x_feats[test])\n",
    "#         print(classification_report(y[test],predicts))\n",
    "    avg_p += precision_score(y[test],predicts, average='macro')\n",
    "    avg_r += recall_score(y[test],predicts, average='macro')\n",
    "    avg_f1 += f1_score(y[test],predicts, average='macro')\n",
    "\n",
    "rf_precision = avg_p/fold\n",
    "rf_recall = avg_r/fold\n",
    "rf_f1 = avg_f1/fold\n",
    "print('Average Precision of RandomForest is %f' % (rf_precision))\n",
    "print('Average Recall is %f.' % rf_recall)\n",
    "print('F1 score is %f.' %(rf_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
